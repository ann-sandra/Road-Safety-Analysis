# -*- coding: utf-8 -*-
"""Accident Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V82_SPUSx2d9jeyY_YMiwQPBhj_-vcdV

# Analysis of accidents in Queensland
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px

# configure glob and search path for files
# Here, we want to search the input directory for CSV files.
# add the path to your unique folder.
data = pd.read_csv("C:\\Users\Praveen Kumar\OneDrive\Desktop\Mobility Eng 2023\Data Science\Assignment 1\crash data\crash_data_queensland_1_crash_locations (2).csv")

pd.set_option('display.max_columns', None)

data.head()

# to see the dtype and the non-null count
data.info()

"""### Bussiness Objective:-

    1. Minimize the accident rates and Maximize the Public Saftey.
    2. Build a Machine Learning Model to Predict the accident sevirety.

### Success Criteria:-
    
    1. ML model should have accuracy of 90%

### Data Understanding:-
    
    1. Our Target Variable will be Crash_Severity and it is categorical in nature.
    2. All other features are also in categorical Type.

# 0.Exploratory Data Analysis (EDA) :-
"""

data.drop(["Loc_Post_Code"],1,inplace = True)

# Load Packages
import sweetviz as sv

# Analyse Dataset
report = sv.analyze(data)

# View and Save
report.show_html()

data.describe()

data.var()==0

data.std()

"""#### Due to categorical nature and uncertainity of accidents occuring, Our data doesn't have good spread or normaly distributed."""

data.skew()

data.kurt()

"""# 0.1.Data Cleaning:

### 0.2.Removing Null Values:-

#### Removing Duplicates and Missing Values :-
"""

data = data.drop_duplicates()

missing_value_proportions = np.round((data.isnull().sum()/len(data))*100,2)

missing_value_df = pd.DataFrame({'Columns': missing_value_proportions.index, 'Missing Value Proportion': missing_value_proportions.values})
fig = go.Figure(data=[go.Table(
    header=dict(values=['Columns', 'Missing Value Proportion']),
    cells=dict(values=[missing_value_df['Columns'], missing_value_df['Missing Value Proportion']])
)])

fig.update_layout(
    title="Missing Value Precentages for 51 Columns"
)

fig.show()

"""#### Removing columns -- "Crash_Street_Intersecting" = 56% null values, "State_Road_Name" = 54% null values,

#### because it contains high volume of missing values if we try to impute, our dataset will become biased.
"""

data.drop(["Crash_Street_Intersecting","State_Road_Name"],1,inplace = True)

data.dropna(inplace = True)

data.isnull().sum()

"""# 1.Summary statistics:

## 1.1.Univariate Analysis :-

### 1.1.1 Number accident recorded:
"""

tl_acc = data["Crash_Severity"].value_counts().sum()
print(f"Total Number of accidents: {tl_acc}")

"""#### Property damage only : 86536."""

data["Crash_Severity"].value_counts().sum() - 86536

"""##### This values means no person injured only property damage was done, Hence we are subtracting it with total accident to find injured people.

##### Total Number of accidents occured was 374214,
##### Total Number of People Got Injuried during an accident was 287678.

### 1.1.2.Crash Severity :
"""

fig = px.histogram(data, x="Crash_Severity", title="Histogram of Crash Severity")
fig.update_xaxes(categoryorder='total ascending')  # Sort x-axis labels
fig.update_xaxes(tickangle=0)  # Rotate x-axis labels for readability

fig.show()

"""##### From this histogramÂ we can find 116782 people receiving medical treatment and nearly 5562 people died as a result of the accident."""

# Calculate the percentage distribution of values
percentage_distribution = (data["Crash_Severity"].value_counts() / data["Crash_Severity"].value_counts().sum()) * 100

fig = px.pie(
    values=percentage_distribution.values,
    names=percentage_distribution.index,
    title="Percentage Distribution of Crash Severity"
)

fig.show()

"""#####   As we can see probality of getting a Fatal Rate is 1.4% ,On other hand Medical treatment & Hospitalisation has 31% + 30% = 61%,

#####   Fortunately 23% of people escaped injuries during an accident.

### 1.1.3. Crash Year
"""

year_wise = data.Crash_Year.value_counts()

# We will calculate the number of accidents per year using this insight:
df = pd.DataFrame(data.Crash_Year.value_counts()).reset_index()
df.columns = ["year","counts"]
df = df.sort_values(by = "year",ascending=True)
df

fig = go.Figure()

fig.add_trace(go.Bar(x=df['year'], y=df['counts'],
                     marker_color='#FF6865',
                     marker_line_color='white',
                     marker_line_width=2))

# Customize layout
fig.update_layout(
    xaxis=dict(title='Year'),
    yaxis=dict(title='Count'),
    title='Year-wise Count',
    width=800,
    height=400
)

# Show the plot
fig.show()

(df.loc[df.year<2011].sum()/10)

"""##### This graph shows total accident counts by year and indicates that the number of accidents was low in 2023 - (79) and the highest accident rate was from 2000 to 2010, there were 22508 accidents on average annually.

"""

df_casuality = pd.DataFrame(data.Count_Casualty_Total.value_counts()).reset_index()

df_casuality.columns = ["no_of_casuality","no_of_Accident_invoved"]

df_casuality = df_casuality.sort_values(by = "no_of_Accident_invoved",ascending=True)

percetage_df_casuality = df_casuality.no_of_Accident_invoved/df_casuality.no_of_Accident_invoved.sum()

percetage_df_casuality.reset_index()

percetage_df_casuality = pd.DataFrame(percetage_df_casuality)

percetage_df_casuality = pd.concat([percetage_df_casuality,df_casuality.no_of_casuality],axis = 1)


plt.figure(figsize=(12, 6))

plt.plot(percetage_df_casuality.no_of_casuality, percetage_df_casuality.no_of_Accident_invoved, marker='o', linestyle='-', color='m')

plt.xlabel("Number of people injured during an accident")

plt.ylabel("probablity of accidents happend")

plt.grid()

"""##### We can say that chance of single person meeting an accident is 59% and 23% chance of no accident,chance for two person meeting an accident is 13%.Other Values are very low."""

# Copying the data
df2 = data.copy()

"""### 1.1.4. Crash_Month:"""

fig = px.pie(df2, names='Crash_Month', title='Distribution of Accidents by Month')

# Show the plot
fig.show()

"""##### Number of Accidents occured by months was perty much same, Hence the proportions also same.

### 1.1.5. Crash_Day_Of_Week :-
"""

fig = px.pie(df2, names='Crash_Day_Of_Week', title='Distribution of Accidents by Day of the Week')

# Show the plot
fig.show()

"""##### As shown in the graph, Friday had the most accidents, making it the most dangerous day.

### 1.1.6.Crash_Hour
"""

hour_wise = df2.Crash_Hour.value_counts()
hour_wise = hour_wise.to_frame().reset_index()
hour_wise.columns = ["hours","Crash_counts"]

hour_wise = hour_wise.sort_values(by = "Crash_counts")
hour_wise

fig = px.bar(hour_wise, x='hours', y='Crash_counts', title='Hour-wise Crash Counts')

fig.update_layout(
    xaxis_title="Hour",
    yaxis_title="Crash Counts"
)

fig.show()

filtered_hour_wise = hour_wise.loc[(hour_wise['hours'] >= 15) & (hour_wise['hours'] <= 17)]

filtered_hour_wise["Crash_counts"].sum()/hour_wise["Crash_counts"].sum()

"""##### At 3pm to 5pm most accidents happend. Average Accidents occured during this period was 30330.
##### Around 24% of Accidents occured in this time period.
"""

df2 = df2.drop(["Crash_Ref_Number"],1)

"""### 1.1.7. Crash_Street :-"""

# Get the top 10 most common values and their counts
Crs_st = df2['Crash_Street'].value_counts().sort_values(ascending=False)[:10]

# Create a bar chart
fig = px.bar(x=Crs_st.index, y=Crs_st, labels={'x': 'Crash Street', 'y': 'Frequency'}, title='Top 10 Most Common Crash Streets')

fig.update_layout(
    xaxis_tickangle=45,  # Rotate x-axis labels for readability
)

fig.show()

"""##### "These are the top ten streets where accidents have occurred in the past. Notably, Bruce Highway has the highest accident rate, and Pacific Highway follows as the second-highest, in comparison to the remaining streets."

##### Identifying streets with higher accident rates can help authorities and transportation agencies focus on improving safety measures, implementing better traffic management, and conducting more in-depth analyses of the factors contributing to accidents on these specific roadways.

### 1.1.8.Crash_Nature :-
"""

# Get the frequency of different crash natures
Crs_nt = df2['Crash_Nature'].value_counts().sort_values(ascending=False)

# Create a bar chart
fig = px.bar(x=Crs_nt.index, y=Crs_nt, labels={'x': 'Crash Nature', 'y': 'Frequency'}, title='Crash Nature')

fig.update_layout(
    xaxis_tickangle=45,  # Rotate x-axis labels for readability
)

fig.show()

"""1. Angle (111,049): Angle accidents typically occur when two vehicles collide at an angle, often at intersections.

2. Rear-end (92,892): Rear-end accidents happen when one vehicle collides with the rear of another vehicle. These accidents are common, often occurring in situations where one vehicle follows too closely or doesn't stop in time.

3. Hit object (81,644): Hit object accidents involve a vehicle colliding with a stationary object, such as a barrier, pole, or tree.

4. Overturned (19,019): Overturned accidents occur when a vehicle flips over or rolls onto its side. These can be quite severe and result in significant damage.

5. Sideswipe (17,663): Sideswipe accidents involve vehicles colliding along their sides. This can happen when two vehicles are traveling in parallel and make contact with each other.

6. Non-collision - miscellaneous (768): This category likely includes accidents that don't involve a direct collision, but   rather non-collision incidents with miscellaneous causes.

7. Struck by external load (479): These accidents involve vehicles being struck by external objects or loads, which can happen in situations like construction zones or when transporting cargo.

8. Collision - miscellaneous (333): This category may encompass collision incidents that do not fit into the more common accident types.

##### Angle,Rear_End and Hit Object are the Top three Accident Nature which is very significant.Struck by external load has very lower accident rate when comparing to other nature types.

### 1.1.9. Crash_Type :-
"""

# Get the frequency of different crash types
Crs_type = df2['Crash_Type'].value_counts().sort_values(ascending=False)

# Create a bar chart
fig = px.bar(x=Crs_type.index, y=Crs_type, labels={'x': 'Crash Type', 'y': 'Frequency'}, title='Crash Type')

fig.update_layout(
    xaxis_tickangle=45,  # Rotate x-axis labels for readability
)

fig.show()

percentage_distribution = (Crs_type / Crs_type.sum()) * 100

# Create a pie chart
fig = px.pie(
    values=percentage_distribution,
    names=percentage_distribution.index,
    title="Percentage Distribution of Crash Types"
)

# Show the plot
fig.show()

print(f"Total number of Pedestrian involved in accidents : {df2.Count_Unit_Pedestrian.sum()}")

"""##### Notably The probablity of accident occured for Multi-Vehicle was around 61% and For Single-Vehicle was around 32%.
##### Total number of Pedestrians met accident was 16635

### 1.1.10. Loc_Suburb :-
"""

# Get the top 10 most common suburbs and their counts
Loc_S = df2['Loc_Suburb'].value_counts().sort_values(ascending=False)[:10]

# Create a bar chart
fig = px.bar(x=Loc_S.index, y=Loc_S, labels={'x': 'Loc_Suburb', 'y': 'Frequency'}, title='Top 10 Most Common Suburbs')

fig.update_layout(
    xaxis_tickangle=45,  # Rotate x-axis labels for readability
)

fig.show()

"""##### These are the Top Suburabs, which has higher accident rate when comparing to others.Sum of Southport, Brisbane_City, Cabooltrue has 41%.

### 1.1.11. Loc_Police_District :-
"""

# Get the frequency of different police districts
Loc_Police_Dist = df2['Loc_Police_District'].value_counts().sort_values(ascending=False)

# Create a bar chart
fig = px.bar(x=Loc_Police_Dist.index, y=Loc_Police_Dist, labels={'x': 'Loc_Police_District', 'y': 'Frequency'}, title='Police Districts')

fig.update_layout(
    xaxis_tickangle=45,  # Rotate x-axis labels for readability
)

fig.show()

"""##### These are the top ten police divisions need to carefully examine accident-prone areas and enhance law enforcement and safety measures to effectively reduce the number of accidents.

### 1.1.12. Crash_Lighting_Condition :-
"""

Crash_Lighting_Condition = df2.Crash_Lighting_Condition.value_counts().sort_values(ascending = False)
# Create a bar plot
plt.figure(figsize=(12, 6))
plt.bar(Crash_Lighting_Condition.index, Crash_Lighting_Condition)
plt.xlabel('Crash_Lighting_Condition')
plt.ylabel('Frequency')
plt.title('Crash_Lighting_Conditiont')
plt.xticks(rotation=45)  # Rotate x-axis labels for readability
plt.show()

"""#### The majority of accidents happen during the day, and the fewest are mysterious.

### 1.1.13. Loc_Main_Roads_Region :-
"""

Loc_Main_Roads_Region = df2.Loc_Main_Roads_Region.value_counts().sort_values(ascending = False)
# Create a bar plot
plt.figure(figsize=(12, 6))
plt.bar(Loc_Main_Roads_Region.index, Loc_Main_Roads_Region)
plt.xlabel('Loc_Main_Roads_Region')
plt.ylabel('Frequency')
plt.title('Loc_Main_Roads_Region')
plt.xticks(rotation=45)  # Rotate x-axis labels for readability
plt.show()

"""#### The accident occurs in Metropolitian is much higher and the lower is Central Queensland."""

# Filter the DataFrame to include rows where Count_Casualty_Hospitalised is greater than 0
Crash_Roadway_Analysis = df2.Crash_Roadway_Feature.value_counts()
Crash_Roadway_Analysis

"""### 1.1.14. Crash_Roadway_Feature :-"""

Crash_Roadway_Feature = df2.Crash_Roadway_Feature.value_counts().sort_values(ascending = False)
# Create a bar plot
plt.figure(figsize=(12,6))
plt.bar(Crash_Roadway_Feature.index, Crash_Roadway_Feature)
plt.xlabel('Crash_Roadway_Feature')
plt.ylabel('Frequency')
plt.title('Crash_Roadway_Feature')
plt.xticks(rotation=45)  # Rotate x-axis labels for readability
plt.show()

"""#### No Roadway Feature has had 198554 accidents, while there are 108 bike accidents."""

# Assuming 'df2' is your DataFrame
feature_distribution_rd = (df2['Crash_Roadway_Feature'].value_counts() / df2['Crash_Roadway_Feature'].count()) * 100

# Create a pie chart
fig = px.pie(names=feature_distribution_rd.index, values=feature_distribution_rd.values, title='Crash Roadway Feature Distribution')

# Show the plot
fig.show()

"""#### The majority of accidentsâroughly 53.1%âoccur in areas without roadway features, while the least amountâbetween 0.4% and 0.0002%âoccurs in median openings, merge lanes, railway crossings, intersections (multiple roads), forestry/national park roads, intersection-y-junctions, and bikeways.

### 1.1.15. Crash_Speed_Limit :-
"""

Crash_Speed_Limit = df2.Crash_Speed_Limit.value_counts().sort_values(ascending = False)
# Create a bar plot
plt.figure(figsize=(12, 6))
plt.bar(Crash_Speed_Limit.index, Crash_Speed_Limit)
plt.xlabel('Crash_Speed_Limit')
plt.ylabel('Frequency')
plt.title('Crash_Speed_Limit')
plt.xticks(rotation=45)  # Rotate x-axis labels for readability
plt.show()

"""### The majority of accidents happen when a vehicle is moving at 60 km/h; on the other hand, the fewest accidents happen when a vehicle is moving at 70 km/h.

### 1.1.16.Crash_Road_Surface_Condition :-
"""

Crash_Road_Surface_Condition = df2.Crash_Road_Surface_Condition.value_counts().sort_values(ascending = False)
# Create a bar plot
plt.figure(figsize=(12, 6))
plt.bar(Crash_Road_Surface_Condition.index, Crash_Road_Surface_Condition)
plt.xlabel('Crash_Road_Surface_Condition')
plt.ylabel('Frequency')
plt.title('Crash_Road_Surface_Condition')
plt.xticks(rotation=45)  # Rotate x-axis labels for readability
plt.show()

"""####

### The majority of accidents occur on sealed dry roads, whereas the minority occur on unsealed wet roads.

### 1.1.17.Crash_Traffic_Control :-
"""

Crash_Traffic_Control = df2.Crash_Traffic_Control.value_counts().sort_values(ascending = False)
# Create a bar plot
plt.figure(figsize=(12, 6))
plt.bar(Crash_Traffic_Control.index, Crash_Traffic_Control)
plt.xlabel('Crash_Traffic_Control')
plt.ylabel('Frequency')
plt.title('Crash_Traffic_Control')
plt.xticks(rotation=45)  # Rotate x-axis labels for readability
plt.show()

Crash_Traffic_Control

"""### The highest accident occurs in No traffice control, whereas accident are quite less in school crossing-flags

####
"""

control_distribution = (df2['Crash_Traffic_Control'].value_counts() / len(df2['Crash_Traffic_Control'])) * 100

# Create a pie chart
fig = px.pie(names=control_distribution.index, values=control_distribution.values, title='Crash Traffic Control Distribution')

# Show the plot
fig.show()

"""####

#### The most accidents are reported in areas without traffic control, while school crossings have the fewest accidents.
"""

filtered_df = df2.groupby("Crash_Roadway_Feature")

filtered_df = filtered_df.Crash_Severity.value_counts().sort_values(ascending = False)

"""## 1.2.Bivariate Analysis :-

### 1.2.1. Crash sevierity -- Fatel vs Mode of Transport :-
"""

ft_analysis = df2.loc[df2.Count_Casualty_Fatality>0]

ft_analysis.head()

fatal_injuries = {

    'Car': ft_analysis['Count_Unit_Car'].sum(),

    'Motorcycle': ft_analysis['Count_Unit_Motorcycle_Moped'].sum(),

    'Truck': ft_analysis['Count_Unit_Truck'].sum(),

    'Bus': ft_analysis['Count_Unit_Bus'].sum(),

    'Pedestrian': ft_analysis['Count_Unit_Pedestrian'].sum()

}


fig = px.pie(

    names=list(fatal_injuries.keys()),

    values=list(fatal_injuries.values()),

    title="Distribution of Fatal Injuries by Unit"

)

fig.show()

"""####

#### Nearly 65.5% of all deaths are caused by cars. Truck and motorcycle, respectively, at 11.8% and 12.9%. Additionally, 8.58% of the accident victims passed away, suggesting that taking the bus, which accounts for 1.11% of fatalities, is a safer option than walking.

### 1.2.2. Crash sevierity -- Casualty_MedicallyTreated vs Mode of Transport :-
"""

# Filter the DataFrame to include rows where Count_Casualty_MedicallyTreated is greater than 0
MT_analysis = df2.loc[df2.Count_Casualty_MedicallyTreated > 0]

# Calculate the counts for each unit
unit_counts = {
    'Car': MT_analysis['Count_Unit_Car'].sum(),
    'Motorcycle': MT_analysis['Count_Unit_Motorcycle_Moped'].sum(),
    'Truck': MT_analysis['Count_Unit_Truck'].sum(),
    'Bus': MT_analysis['Count_Unit_Bus'].sum(),
    'Pedestrian': MT_analysis['Count_Unit_Pedestrian'].sum()
}

# Create a bar chart
fig = px.bar(
    x=list(unit_counts.keys()),
    y=list(unit_counts.values()),
    labels={'x': 'Unit', 'y': 'Count'},
    title='Casualties Medically Treated by Unit'
)

fig.show()

"""####

#### Similar to the death rate, the majority of car passengers had accidents and required medical attention. After this, there are roughly 10.481k and 8.840k motorcycle and truck riders at the time of medical treatment, respectively, and about 2607 bus riders, which is fewer than pedestrians.

### 1.2.3.Casualty_Hospitalised vs Mode of Transport :-
"""

Hos_analysis = df2.loc[df2.Count_Casualty_Hospitalised>0]
hospitalized_vehicle_counts = {
    'Car': Hos_analysis['Count_Unit_Car'].sum(),
    'Motorcycle': Hos_analysis['Count_Unit_Motorcycle_Moped'].sum(),
    'Truck': Hos_analysis['Count_Unit_Truck'].sum(),
    'Bus': Hos_analysis['Count_Unit_Bus'].sum(),
    'Pedestrian': Hos_analysis['Count_Unit_Pedestrian'].sum(),
}

# Create a pie chart
fig = px.pie(names=hospitalized_vehicle_counts.keys(), values=hospitalized_vehicle_counts.values(),
             title='Count of Hospitalized Casualties by Vehicle Type')

# Show the plot
fig.show()

"""####

### 1.2.4. Casualty_MinorInjury vs Mode of Transport :-
"""

minority_analysis = df2.loc[df2['Count_Casualty_MinorInjury']>0]
minority_analysis_vehicle_counts = {
    'Car': minority_analysis['Count_Unit_Car'].sum(),
    'Motorcycle': minority_analysis['Count_Unit_Motorcycle_Moped'].sum(),
    'Truck': minority_analysis['Count_Unit_Truck'].sum(),
    'Bus': minority_analysis['Count_Unit_Bus'].sum(),
    'Pedestrian': minority_analysis['Count_Unit_Pedestrian'].sum(),
}

# Create a pie chart
fig = px.pie(names=minority_analysis_vehicle_counts.keys(), values=minority_analysis_vehicle_counts.values(),
             title='Count of Minor Injured Casualties by Vehicle Type')

# Show the plot
fig.show()

"""#### The visualizations presented above emphasize the significant contribution of car-related incidents in overall accidents. The data strongly suggests that opting for public transportation may be a safer choice for commuters. By relying on public transit, individuals can potentially reduce their exposure to the risks associated with car accidents. This insight underscores the importance of considering alternative modes of transportation to enhance overall safety on the roads.

###

### 1.2.5. Analysing Relationship between these three features Crash_Roadway_Feature,Crash_Severity, Crash Street
"""

# Group by "Crash_Roadway_Feature" and calculate value counts for "Crash_Severity"
grouped_df = df2.groupby("Crash_Roadway_Feature")["Crash_Severity"].value_counts().sort_values(ascending=False).reset_index(name='Count')

# Use Plotly Express to create a bar chart
fig = px.bar(grouped_df, x='Crash_Roadway_Feature', y='Count', color='Crash_Severity',
             labels={'Count': 'Number of Crashes'},
             title='Distribution of Crash Severity by Roadway Feature',
             barmode='stack',  # Use 'stack' for stacked bars
             hover_data=['Crash_Severity'])  # Add hover data for main chart

# Group by "Crash_Roadway_Feature" and "Crash_Street" and calculate value counts for "Crash_Severity"
grouped_streets_df = df2.groupby(["Crash_Roadway_Feature", "Crash_Street"])["Crash_Severity"].value_counts().sort_values(ascending=False).reset_index(name='Count')

# Keep the top 2 streets for each "Crash_Roadway_Feature"
top_streets_df = grouped_streets_df.groupby("Crash_Roadway_Feature").head(2)

# Use Plotly Express to create a horizontal bar chart for top streets
fig_streets = px.bar(top_streets_df, x='Count', y='Crash_Street', color='Crash_Severity',
                     labels={'Count': 'Number of Crashes'},
                     title='Top Crash Streets by Crash Severity for Each Roadway Feature',
                     orientation='h',
                     hover_data=['Crash_Severity', 'Crash_Roadway_Feature'])  # Add hover data for top streets

# Customize the layout if needed
fig_streets.update_layout(
    barmode='stack',  # Use 'stack' for stacked bars
    height=800,       # Set the height
    width=1200,       # Set the width
)

# Set marker line color and width
fig_streets.update_traces(marker_line_color='black', marker_line_width=1.5)

# Show the plots
fig.show()
fig_streets.show()

"""#### Crash Severity by Roadway feature:
As was previously observed, No_roadway_feature has more accidents. Now, a detailed examination of No_roadway_feature with Crash_severity shows that hospitalisation has the highest count in No_roadway_feature, and this medical treatment is continued. In addition, the lowest crash severity in the no roadway feature is the fatality rate.

#### Top Crash Streets by Crash Severity for Each Roadway Feature:
As we seen before Bruce Hwy have the number of crash about 4505 in No roadway feature and they were hospitalised.Following this in T-jusction Bruce Hwy have 688 crashes and they where hospitalised.The least accident occurs in Railway crossing is about 17 in number and they where

## 1.3.An overview of road accidents in Queensland is provided here:

1. **Crash Severity:**
   - 116,782 people received medical treatment.
   - Approximately 5,562 fatalities occurred due to accidents.

2. **Crash Year:**
   - Lowest accident count in 2023 (79 incidents).
   - Highest accident rate observed from 2000 to 2010, averaging 22,508 accidents annually.

3. **Crash Probability:**
   - 59% chance of a single person encountering an accident.
   - 23% chance of no accident.
   - 13% chance of two people meeting an accident.

4. **Crash Month:**
   - Similar accident counts across months, indicating proportional distribution.

5. **Crash Day of the Week:**
   - Friday had the highest number of accidents, making it the most dangerous day.

6. **Crash Hour:**
   - Most accidents occurred from 3 pm to 5 pm.
   - Average accidents during this period were 30,330, constituting around 24% of total accidents.

7. **Crash Street:**
   - Identified top ten streets with Bruce Highway having the highest accident rate.
   - Suggested focusing on improving safety measures on these specific roadways.

8. **Crash Nature:**
   - Top three accident natures: Angle, Rear-end, and Hit Object.
   - Struck by external load had a lower accident rate compared to other types.

9. **Crash Type:**
   - Probability of multi-vehicle accidents was around 61%.
   - Probability of single-vehicle accidents was around 32%.
   - Total pedestrian accidents: 16,635.

10. **Location Suburb:**
    - Top suburbs with higher accident rates: Southport, Brisbane City, Caboolture (totaling 41%).

11. **Location Police District:**
    - Identified top ten police districts for enhanced law enforcement and safety measures.

12. **Crash Lighting Condition:**
    - Majority of accidents occurred during the day.

13. **Location Main Road Region:**
    - Higher accident rates in Metropolitan areas compared to Central Queensland.

14. **Crash Roadway Feature:**
    - No Roadway Feature had the highest accident count, while bike accidents were comparatively low (108 incidents).

15. **Crash Speed:**
    - Majority of accidents occurred at 60 km/h.

16. **Crash Road:**
    - Majority of accidents occurred on sealed dry roads.

17. **Crash Traffic Control:**
    - Highest accident rates in areas with no traffic control, lower rates in school crossing-flag areas.

This comprehensive analysis provides insights into various aspects of crash data, aiding in informed decision-making for improving road safety.

# 2.Feature Distribution:

### 2.1.Outliers Analysis :-
"""

figure, axis = plt.subplots(3, 4, figsize=(30,12))
axis[0, 0].hist(df2["Count_Casualty_Fatality"],color = "red")
axis[0, 0].set_title("Count_Casualty_Fatality")
axis[0, 1].hist(df2["Count_Casualty_Hospitalised"],color = "blue")
axis[0, 1].set_title("Distribution of person type")
axis[0, 2].hist(df2["Count_Casualty_Hospitalised"],color = "orange")
axis[0, 2].set_title("Count_Casualty_MedicallyTreated")
axis[0, 3].hist(df2["Count_Casualty_MinorInjury"],color = "purple")
axis[0, 3].set_title("Count_Casualty_MinorInjury")

axis[1, 0].hist(df2["Count_Casualty_Total"],color = "yellow")
axis[1, 0].set_title("Count_Casualty_Total")
axis[1, 1].hist(df2["Count_Unit_Car"],color = "green")
axis[1, 1].set_title("Count_Unit_Car")
axis[1, 2].hist(df2["Count_Unit_Motorcycle_Moped"],color = "brown")
axis[1, 2].set_title("Count_Unit_Motorcycle_Moped")
axis[1, 3].hist(df2["Count_Unit_Truck"],color = "crimson")
axis[1, 3].set_title("Count_Unit_Truck")

axis[2, 0].hist(df2["Count_Unit_Bus"],color = "violet")
axis[2, 0].set_title("Count_Unit_Bus")
axis[2, 1].hist(df2["Count_Unit_Bicycle"],color = "Turquoise")
axis[2, 1].set_title("Count_Unit_Bicycler")
axis[2, 2].hist(df2["Count_Unit_Pedestrian"],color = "red")
axis[2, 2].set_title("Count_Unit_Pedestrian")
axis[2, 3].hist(df2["Count_Unit_Other"],color = "black")
axis[2, 3].set_title("Count_Unit_Other")

column_names = ['Count_Casualty_Hospitalised', 'Count_Casualty_MedicallyTreated', 'Count_Casualty_Total','Count_Unit_Car']

# Calculate the IQR (Interquartile Range) for each column
IQRs = {}
for column_name in column_names:
    Q1 = df2[column_name].quantile(0.25)
    Q3 = df2[column_name].quantile(0.75)
    IQRs[column_name] = Q3 - Q1

# Identify outliers for each column
outliers_dict = {}
for column_name in column_names:
    IQR = IQRs[column_name]
    outliers_dict[column_name] = df2[(df2[column_name] < (Q1 - 1.5 * IQR)) | (df2[column_name] > (Q3 + 1.5 * IQR))]

# Create box plots to visualize outliers for each column
for column_name in column_names:
    plt.boxplot(df2[column_name])
    plt.xlabel(column_name)
    plt.ylabel('Value')
    plt.title('Box Plot of {}'.format(column_name))
    plt.show()

"""##### After eliminating null values and duplicates from our dataset, the subsequent step involves identifying potential outliers. However, it's essential to note that the treatment of outliers may not be as applicable for categorical data, given its distinct nature. In the realm of categorical features, the concept of outliers is less straightforward than in numerical data.

## 3.Feature Correlations:-
"""

def correlation(dataset, threshold):
    col_corr = set() # set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i,j]) > threshold: # considering the absolute coeff values
                colname = corr_matrix.columns[i] # getting the name of column
                col_corr.add(colname)
    return col_corr

plt.figure(figsize=(26,22))
df2corr = df2.corr(method="pearson")
sns.heatmap(df2corr, annot=True, cmap='inferno')
plt.title("corr matrix")
plt.show()
df2.corr()

corr_features = correlation(df2,0.5)
len(set(corr_features))

correlation(df2,0.5)

"""#### Highest Correlation:
 1) About -0.6 is the highest negative correlation found between Crash_Latitude and Crash_Longitude.Consequently, a change in latitude will effect on longitude.
 2) The relationship between Count_Casualty_Total and Count_Casualty_MedicallyTreated has a positive highest correlation of roughly 0.5, meaning that as medical treatment increases, so does the number of casualties.


#### Medium Correlation:
 1) The Crash_DCA_Code has nagative medium correlation with Count_Unit_Car is about -0.45.
 2) The Crash_DCA_Code has nagative medium correlation with Count_Unit_Pedestrian is about -0.30.
 3) The Count_Casualty_Hospitalised has positive medium correlation with Count_Casualty_Total is about 0.49.
 4) The Count_Casualty_MinorInjury has postive medium correlation with Count_Casualty_Total is about 0.37.
 5) The Count_Unit_Motorcycle_Moped has nagative medium correlation with Count_Unit_Car is about -0.37.

#### Low Correlation:
 1) There is a negative low correlation between the Crash_Year and the Count_Casualty_Hospitalized, indicating a decline in both accident and hospital admission rates.
 2) The Crash_Year and Count_Casualty_Total have a poor correlation, indicating a decline in accidents annually.
 3) The Crash_HourÂ exhibits the least amount of both positive and negative correlation with all columns.
 4) The Crash_Longitude are weekly postive and nagative correlated with all the columns except Crash_Latitude.
 5) The columns with the highest weekly correlations are Count_Casualty_Fatality, which indicates that there is a decline in the rate of fatality count and that not all accidents have been mapped to death rates.
 6) There is a lower number of people being hospitalised for medical treatment, as indicated by the low positive correlation between Count_Casualty_Hospitalised and Count_Casualty_MedicallyTreated.
 7) There is a low negative correlation between Count_Casualty_MinorInjury and Count_Casualty_Hospitalized, indicating a decrease in the number of patients admitted to hospitals for minor injuries.
 8) People with minor injuries receive less medical attention, as seen by the positively low correlation between Count_Casualty_MinorInjuries and Count_Casualty_MedicallyTreated.
"""

hm = df2.copy()
hm = df2.drop(['Crash_Year','Crash_Hour','Count_Casualty_Fatality','Count_Unit_Motorcycle_Moped','Count_Unit_Truck','Count_Unit_Bus','Count_Unit_Bicycle','Count_Unit_Other'],1)

plt.figure(figsize=(26,22))
hmcorr = hm.corr(method="pearson")
sns.heatmap(hmcorr, annot=True, cmap='inferno')
plt.title("corr matrix")
plt.show()
hm.corr()

"""# 4.Time series aspects:"""

My_data = data
data = My_data
# Convert 'Crash_Year', 'Crash_Month', and 'Crash_Day_Of_Week' to datetime
data['Crash_Date'] = pd.to_datetime(data['Crash_Year'].astype(str) + '-' + data['Crash_Month'].astype(str) + '-' + data['Crash_Day_Of_Week'].astype(str),  format='%Y-%B-%A')

# Identify the lockdown period
lockdown_start_date = pd.to_datetime('2020-01-01')
lockdown_end_date = pd.to_datetime('2021-04-01')

# Segregate data before and during lockdown
before_lockdown = data[data['Crash_Date'] < lockdown_start_date]
during_lockdown = data[(data['Crash_Date'] >= lockdown_start_date) & (data['Crash_Date'] <= lockdown_end_date)]
after_lockdown = data[data['Crash_Date'] > lockdown_end_date]

# Calculate the number of accidents and severity before and during lockdown
accidents_before_lockdown = len(before_lockdown)
accidents_during_lockdown = len(during_lockdown)
accidents_after_lockdown = len(after_lockdown)

severity_before_lockdown = before_lockdown['Crash_Severity'].value_counts()
severity_during_lockdown = during_lockdown['Crash_Severity'].value_counts()
severity_after_lockdown = after_lockdown['Crash_Severity'].value_counts()

print(f'Number of accidents before lockdown: {accidents_before_lockdown}')
print(f'Number of accidents during lockdown: {accidents_during_lockdown}')
print(f'Number of accidents after lockdown: {accidents_after_lockdown}')
print("**********\n")


print('Severity Before Lockdown:', severity_before_lockdown)
print("********** \n")

print('Severity During Lockdown:' , severity_during_lockdown)
print("********** \n")

print('Severity After Lockdown:' , severity_after_lockdown)

a=data.Crash_Month.value_counts()

a
autumn=a['October']+a['November']+a['December']
summer=a['July']+a['August']+a['September']
spring=a['April']+a['May']+a['June']
winter= a['January']+a['February']+a['March']

season_data=pd.DataFrame([spring,summer,autumn,winter],index=['Spring','Summer','Autumn','Winter'],
columns=['Total accidents'])
season_data

sns.barplot(season_data.index,season_data['Total accidents'])
plt.xlabel('Seasons')
plt.ylabel('Total accidents')
plt.title('Accidents per Season')

"""### According to the chart most accidents happened in the summer and the least accidents happend in the winter"""

daily_data=pd.DataFrame(data.Crash_Day_Of_Week.value_counts())
daily_data

sns.barplot(daily_data.index,daily_data['Crash_Day_Of_Week'])
plt.xlabel('Day of the Week')
plt.ylabel('Total accidents')
plt.title('Accidents per Day')

data.Crash_Day_Of_Week.value_counts()

"""#### As shown in the graph, Friday had the most accidents, making it the most dangerous day."""

import matplotlib.pyplot as plt
hour_data = data['Crash_Hour'].value_counts().sort_index()

Hour_data = pd.DataFrame(hour_data)
Hour_data = Hour_data.sort_index()
hour_data

plt.figure(figsize=(6, 3))
colors = ['red' if x in [15] else 'lightblue' for x in Hour_data.index]

plt.bar(Hour_data.index, Hour_data.iloc[:, 0], color=colors)
plt.xlabel('Hour of the Day')
plt.ylabel('Total accidents')
plt.title('Accidents per Hour')
plt.show()

"""#### At 3 pm most accidents happend"""

# Data
severity_labels = ['Hospitalisation', 'Property damage only', 'Minor injury', 'Medical treatment', 'Fatal']

fig = go.Figure()

# Before Lockdown
fig.add_trace(go.Bar(x=severity_labels, y=severity_before_lockdown.reindex(severity_labels, fill_value=0),
                     name='Before Lockdown', marker=dict(color='rgba(255, 0, 0, 0.5)')))

# During Lockdown
fig.add_trace(go.Bar(x=severity_labels, y=severity_during_lockdown.reindex(severity_labels, fill_value=0),
                     name='During Lockdown', marker=dict(color='rgba(0, 255, 0, 0.5)')))

# After Lockdown
fig.add_trace(go.Bar(x=severity_labels, y=severity_after_lockdown.reindex(severity_labels, fill_value=0),
                     name='After Lockdown', marker=dict(color='rgba(0, 0, 255, 0.5)')))

# Layout
fig.update_layout(
    title='Severity of Accidents in Different Periods',
    xaxis=dict(title='Severity'),
    yaxis=dict(title='Count'),
    barmode='group'
)

# Show plot
fig.show()

"""##### Based on the bar charts and analysis outputs, it appears that the number of accidents decreased notably during the lockdown period compared to both pre-lockdown and post-lockdown periods. Additionally, the severity of accidents across all types exhibited a substantial decline during this specific timeframe. This observation suggests a potential correlation between the lockdown measures and a significant reduction in both the frequency and severity of traffic accidents."""

import matplotlib.pyplot as plt
pre_lockdown_accidents = before_lockdown.shape[0]
during_lockdown_accidents = during_lockdown.shape[0]
post_lockdown_accidents = after_lockdown.shape[0]
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
avg_before = accidents_before_lockdown/19
avg_during = accidents_during_lockdown/(1.25)
avg_after = accidents_after_lockdown/(2+(1/12))
bars=plt.bar(['Before Lockdown', 'During Lockdown', 'After Lockdown'], [avg_before, avg_during, avg_after])
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval), ha='center', va='bottom')

plt.title('Number of Accidents in Different Periods')
plt.xlabel('Period')
plt.ylabel('Number of Accidents')

plt.tight_layout()
plt.show()

# Plot the severity of accidents in each period
plt.subplots(1, 1, figsize=(15, 12))
severity_labels = ['Hospitalisation', 'Property damage only', 'Minor injury','Medical treatment', 'Fatal']
avg_severity_before = severity_before_lockdown/19
avg_severity_during = severity_during_lockdown/(1.25)
avg_severity_after = severity_after_lockdown/(2+(1/12))
plt.bar(severity_labels, avg_severity_before.reindex(severity_labels, fill_value=0), alpha=1, label='before-Lockdown')
plt.bar(severity_labels, avg_severity_during.reindex(severity_labels, fill_value=0), alpha=1, label='During Lockdown')
plt.bar(severity_labels, avg_severity_after.reindex(severity_labels, fill_value=0), alpha=1, label='after-Lockdown')
plt.title('Severity of Accidents in Different Periods')
plt.xlabel('Severity')
plt.ylabel('Count')
plt.legend()

plt.tight_layout()
plt.show()

"""##### based on the bar chart and outputs, during the lockdown, there were fewer accidents compared to both before and after the lockdown period. In addition, the all types of severity of accidents decreases during this time.

# 5.Spatial Characteristics:
"""

import folium

import pandas as pd
from sklearn.cluster import KMeans

def cluster_crashes(df, n_clusters=1000):
    coords = df[['Crash_Latitude', 'Crash_Longitude']]
    kmeans = KMeans(n_clusters=n_clusters)
    kmeans.fit(coords)
    centers = kmeans.cluster_centers_
    df['cluster'] = kmeans.labels_
    cluster_df = pd.DataFrame(centers, columns=['cluster_lat', 'cluster_long'])
    cluster_counts = df.groupby('cluster').size().reset_index(name='number')
    cluster_df = cluster_df.merge(cluster_counts, left_index=True, right_on='cluster')
    cluster_df.drop('cluster', axis=1, inplace=True)
    return cluster_df

import folium
from folium.plugins import HeatMap

def create_heatmap(df, lat_col='Crash_Latitude', long_col='Crash_Longitude', zoom_start=5):
    crash_map = folium.Map(location=[df[lat_col].mean(), df[long_col].mean()], zoom_start=zoom_start)
    heat_data = [[row[lat_col], row[long_col]] for index, row in df.iterrows()]

    HeatMap(heat_data).add_to(crash_map)

    return crash_map

import pandas as pd
from sklearn.cluster import KMeans

def cluster_crashes(df, n_clusters=1000):
    coords = df[['Crash_Latitude', 'Crash_Longitude']]
    kmeans = KMeans(n_clusters=n_clusters)
    kmeans.fit(coords)
    centers = kmeans.cluster_centers_
    df['cluster'] = kmeans.labels_
    cluster_df = pd.DataFrame(centers, columns=['cluster_lat', 'cluster_long'])
    cluster_counts = df.groupby('cluster').size().reset_index(name='number')
    cluster_df = cluster_df.merge(cluster_counts, left_index=True, right_on='cluster')
    cluster_df.drop('cluster', axis=1, inplace=True)
    return cluster_df

cluster_df = cluster_crashes(data)

cluster_df.columns = ['Crash_Latitude', 'Crash_Longitude', 'number']
map = create_heatmap(cluster_df)
map

"""##  <span style="text-decoration: underline;">Loc Suburban:</span>


#### South Port:
Most of accident happened at the Loc Suburban South port. This location contains four intersections, which may increase the risk of accidents, particularly if driving is difficult.Accidents involving pedestrians and vehicles may occur more frequently in places where there is a lot of foot traffic, such as shopping centres or entertainment venues.The risk of accidents may increase if there are problems with speeding, aggressive driving, or breaking traffic laws. We can prevent this by changing the way that traffic moves, providing a free path for pedestrians, enforcing strict traffic laws, and avoiding speed limits.
https://www.google.com/maps/@-27.9721656,153.4058937,3a,75y,0.29h,67.73t/data=!3m7!1e1!3m5!1svXzIEBXtSDFpt1z4MMq5tg!2e0!6shttps:%2F%2Fstreetviewpixels-pa.googleapis.com%2Fv1%2Fthumbnail%3Fpanoid%3DvXzIEBXtSDFpt1z4MMq5tg%26cb_client%3Dsearch.revgeo_and_fetch.gps%26w%3D96%26h%3D64%26yaw%3D311.5303%26pitch%3D0%26thumbfov%3D100!7i16384!8i8192?entry=ttu

#### Morayfield:
Morayfield has fewer accidents than other areas because it has a lower population density and traffic volume than urban centres, which reduces the likelihood of accidents,Â roadway layouts and intersections that are simpler, which will simplify traffic patterns and lower the risk of accidents. This lowers the chance of collisions involving pedestrians and cars because there are fewer pedestrians and less foot traffic.
https://www.google.com/maps/@-27.1290559,152.9395842,3a,75y,71.17h,86.38t/data=!3m7!1e1!3m5!1sV-ec6MfaZMHrZF8J0ZMbiQ!2e0!6shttps:%2F%2Fstreetviewpixels-pa.googleapis.com%2Fv1%2Fthumbnail%3Fpanoid%3DV-ec6MfaZMHrZF8J0ZMbiQ%26cb_client%3Dsearch.revgeo_and_fetch.gps%26w%3D96%26h%3D64%26yaw%3D114.56888%26pitch%3D0%26thumbfov%3D100!7i16384!8i8192?entry=ttu

##  <span style="text-decoration: underline;">Street:</span>

#### Bruce Highway:
The Bruce Highway has a high accident rate. Highways outside of cities may cross through wildlife habitats in certain places. This makes it more likely that vehicles and wildlife will collide, endangering both groups. Highway-served rural areas may have fewer options for private vehicle travel, which raises the number of vehicles on the road and the risk of accidents.
https://www.google.com/maps/@-22.0536346,149.4888176,3a,75y,116.95h,90t/data=!3m7!1e1!3m5!1sPc9cBUPmvK146IITER8UYg!2e0!6shttps:%2F%2Fstreetviewpixels-pa.googleapis.com%2Fv1%2Fthumbnail%3Fpanoid%3DPc9cBUPmvK146IITER8UYg%26cb_client%3Dsearch.revgeo_and_fetch.gps%26w%3D96%26h%3D64%26yaw%3D116.95064%26pitch%3D0%26thumbfov%3D100!7i16384!8i8192?entry=ttu

#### New England Highway:
In general, drivers are less distracted in rural settings than in urban ones. Reduced traffic and signage may make drivers less distracted and allow them to concentrate more on the road. Rural areas usually have lower traffic volumes because of their lower population densities. Less traffic can mean fewer chances for collisions.
https://www.google.com/maps/place/New+England+Hwy,+Warwick+QLD+4370,+Australia/@-32.79101,151.6296781,3a,75y,87.65h,90t/data=!3m7!1e1!3m5!1s5VGQkitWPk1akvrJ1_aCSA!2e0!6shttps:%2F%2Fstreetviewpixels-pa.googleapis.com%2Fv1%2Fthumbnail%3Fpanoid%3D5VGQkitWPk1akvrJ1_aCSA%26cb_client%3Dsearch.gws-prod.gps%26w%3D211%26h%3D120%26yaw%3D87.6489%26pitch%3D0%26thumbfov%3D100!7i16384!8i8192!4m15!1m8!3m7!1s0x6b73412299f2dbb5:0x96d7b64ca554e568!2sNew+England+Hwy,+Warwick+QLD+4370,+Australia!3b1!8m2!3d-28.2165229!4d152.0348693!16zL20vMDJ3bjk0!3m5!1s0x6b73412299f2dbb5:0x96d7b64ca554e568!8m2!3d-28.2165229!4d152.0348693!16zL20vMDJ3bjk0?entry=ttu

# 6.Contributing factors and accident severity:

### 6.1.Atmospheric Condition affect the accident:
"""

import matplotlib.pyplot as plt
import seaborn as sns

atmospheric_conditions = data["Crash_Atmospheric_Condition"].unique()

plt.figure(figsize=(12, 6))
sns.countplot(x="Crash_Atmospheric_Condition", data=data, order=atmospheric_conditions)
plt.title('Number of Accidents by Atmospheric Condition')
plt.xlabel('Atmospheric Condition')
plt.ylabel('Number of Accidents')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(x="Crash_Atmospheric_Condition", hue="Crash_Severity", data=data, order=atmospheric_conditions)
plt.title('Number of Accidents by Atmospheric Condition and Severity')
plt.xlabel('Atmospheric Condition')
plt.ylabel('Number of Accidents')
plt.xticks(rotation=45)
plt.legend(title='Crash Severity')
plt.show()



"""### 6.2.Vehicle type involved in the most accidents:"""

vehicle_columns = [
    "Count_Unit_Car",
    "Count_Unit_Motorcycle_Moped",
    "Count_Unit_Truck",
    "Count_Unit_Bus",
    "Count_Unit_Bicycle",
    "Count_Unit_Pedestrian",
    "Count_Unit_Other"
]

vehicle_counts = data[vehicle_columns].sum()
plt.figure(figsize=(12, 6))
sns.set_theme(style="whitegrid")
vehicle_plot = sns.barplot(x=vehicle_counts.index, y=vehicle_counts.values, palette="viridis")
vehicle_plot.set_title("Number of Accidents by Vehicle Type")
vehicle_plot.set_xlabel("Vehicle Type")
vehicle_plot.set_ylabel("Number of Accidents")
vehicle_plot.set_xticklabels(vehicle_plot.get_xticklabels(), rotation=45, horizontalalignment='right')
plt.show()

most_dangerous_vehicles = vehicle_counts.sort_values(ascending=False)
print("Most Dangerous Vehicle Types:")
print(most_dangerous_vehicles)

"""### From this information, it appears that situations where all types of vehicles are "car" have the highest sum of Count_Crashes,making it the vehicle type involved in the most accidents.

### 6.3.Deadly accident:
"""

crash_type= pd.read_csv("C:\\Users\Praveen Kumar\OneDrive\Desktop\Mobility Eng 2023\Data Science\Assignment 1\crash data\crash_data_queensland_e_alcohol_speed_fatigue_defect.csv")
crash_type['Crash_category'] = 'other'
crash_type.loc[(crash_type['Involving_Drink_Driving'] == 'Yes'), 'Crash_Category'] = 'Drink'
crash_type.loc[(crash_type['Involving_Driver_Speed'] == 'Yes'), 'Crash_Category'] = 'Speed'
crash_type.loc[(crash_type['Involving_Fatigued_Driver'] == 'Yes'), 'Crash_Category'] = 'Fatigued'
crash_type.loc[(crash_type['Involving_Defective_Vehicle'] == 'Yes'), 'Crash_Category'] = 'Defective'
crash_type.loc[(crash_type['Involving_Drink_Driving'] == 'No') & (crash_type['Involving_Driver_Speed'] == 'No') & (crash_type['Involving_Fatigued_Driver'] == 'No') & (crash_type['Involving_Defective_Vehicle'] == 'No'), 'Crash_Category'] = 'other_Reason'

crash_type = crash_type[crash_type['Crash_Category'] != 'other_Reason']
casualty_stats = crash_type.groupby('Crash_Category').agg({
    'Count_Fatality': 'sum'

})


casualty_stats['Fatality_Rate'] = (
    casualty_stats['Count_Fatality'] / sum(crash_type['Count_Fatality'])*100
)


print(round(casualty_stats['Fatality_Rate'],2).astype(str) + ' %')

most_deadly =casualty_stats['Fatality_Rate'].idxmax()

print(f"The most deadly type of accident is: {most_deadly}")

labels = casualty_stats.index
sizes = casualty_stats['Fatality_Rate']


plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, autopct='%1.2f%%', startangle=90, colors=plt.cm.Paired.colors)
plt.title('Fatality Rates by Crash Category')
plt.show()

"""## Based on the provided data on the Fatality Rate by Crash Category,Two types of accidents are more deadly than others:
##### Speed-Related Accidents: The highest fatality rate is associated with accidents related to speed, accounting for approximately 21.66%. This suggests that accidents involving high speeds are more likely to result in fatalities.
##### Drink-Related Accidents: Accidents related to alcohol consumption (Drink) also contribute significantly to the fatality rate, representing about 11.49%. This highlights the dangers of driving under the influence of alcohol. #Recommendations:
##### Speed Management: Implementing measures to control and monitor vehicle speeds can significantly contribute to reducing the fatality rate. This might include speed limits, traffic calming measures, and public awareness campaigns.
##### Anti-Drinking Campaigns: Strengthening campaigns and enforcement against driving under the influence of alcohol can be effective in reducing fatalities related to drink-related accidents.
##### Addressing Driver Fatigue: Initiatives to address driver fatigue, such as rest area  facilities, awareness programs, and regulation of driving hours, can contribute to reducing fatalities in fatigued-related accidents.
"""

crashes_data=pd.read_csv("C:\\Users\Praveen Kumar\OneDrive\Desktop\Mobility Eng 2023\Data Science\Assignment 1\crash data\crash_data_queensland_1_crash_locations (2).csv")
road_curve_analysis = crashes_data[['Crash_Road_Horiz_Align', 'Count_Casualty_Total']].groupby(['Crash_Road_Horiz_Align']).mean()
road_curve_analysis = road_curve_analysis.reset_index()
road_curve_analysis.rename(columns={'Count_Casualty_Total': 'Mean_Casualty_Total'}, inplace=True)
road_curve_analysis = road_curve_analysis.iloc[:-1]

road_curve_analysis.head(20)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
colors = ['blue' if align != 'Straight' else 'green' for align in road_curve_analysis['Crash_Road_Horiz_Align']]

plt.bar(road_curve_analysis['Crash_Road_Horiz_Align'], road_curve_analysis['Mean_Casualty_Total'], color=colors)
plt.title('Mean Casualty Total by Road Alignment')
plt.xlabel('Road Alignment')
plt.ylabel('Mean Casualty Total')
plt.xticks(rotation=45, ha='right')
plt.show()

"""#### In comparison to the average number of accidents, the number of accidents on curve roads is twice as high as on straight roads.
However, the difference is relatively small,even a small increase in casualties can be a cause for concern from a public safety perspective. It might warrant further investigation and potential interventions to enhance road safety.In summary, while 2% may seem small in some contexts, the significance depends on various factors, including statistical considerations, domain-specific criteria, and the potential real-world impact on safety. Further analysis, including statistical testing and consultation with experts in traffic safety, would provide a more nuanced understanding of the findings and additional investigations, considering factors like road conditions, speed limits, and traffic density, would be necessary for a more comprehensiveÂ assessment.
"""

ped_data =data[data["Crash_Type"] == "Hit pedestrian"]
grouped = ped_data.groupby("Crash_Roadway_Feature").count()
crashes = grouped["Crash_Ref_Number"]
crashes

plt.figure(figsize=(10, 6))
sns.barplot(x=crashes.values, y=crashes.index, palette='viridis')
plt.title('Distribution of Pedestrian Crashes Based on Roadway Features')
plt.xlabel('Number of Crashes')
plt.ylabel('Roadway Feature')
plt.show()

pedestrian_data = crashes_data[crashes_data["Crash_Type"] == 'Hit pedestrian']
hours_distribution = pedestrian_data['Crash_Hour'].value_counts()
print("\nDistribution of Hour_data :\n", hours_distribution.sort_index())
plt.figure(figsize=(12, 6))
sns.barplot(x=hours_distribution.index, y=hours_distribution.values, palette="viridis")
plt.title('Distribution of Pedestrian Crashes by Hour')
plt.xlabel('Crash Hour')
plt.ylabel('Count')
plt.show()

age_data = pd.read_csv("C:\\Users\Praveen Kumar\OneDrive\Desktop\Mobility Eng 2023\Data Science\Assignment 1\crash data\crash_data_queensland_a_road_casualties.csv",low_memory=False )
ped_age = age_data[age_data["Casualty_RoadUserType"] == "Pedestrian"]
ped_age = ped_age.groupby("Casualty_AgeGroup").count()["Casualty_Count"].iloc[:-1]

ped_age

ped_age_data = {
    "0 to 16": 861,
    "17 to 24": 792,
    "25 to 29": 596,
    "30 to 39": 736,
    "40 to 49": 701,
    "50 to 59": 638,
    "60 to 74": 635,
    "75 and over": 526
}

ages = list(ped_age_data.keys())
counts = list(ped_age_data.values())

plt.figure(figsize=(10, 6))
plt.barh(ages, counts, color='skyblue')
plt.xlabel('Count')
plt.ylabel('Age Group')
plt.title('Pedestrian Casualties by Age Group')
plt.show()

"""## To derive safety recommendations for pedestrians based on your analysis, you can consider the following insights:

### Crash_Lighting_Condition Analysis:
   - **Darkness - Not lighted:** Pedestrian accidents are reported in conditions with no lighting. Enhance street lighting, especially in areas with pedestrian traffic during the night, to improve visibility.

### Combined Analysis of Crash_Type and Crash_Lighting_Condition and Crash_Atmospheric_Condition:
   - Observe the distribution of pedestrian accidents concerning lighting conditions and crash types. For instance, if a significant number of pedestrian accidents occur in Darkness - Not lighted conditions, focus on improving lighting in those areas.
   - Additionally, measures to enhance visibility during dawn/dusk conditions could be considered.

### Crash_Roadway_Feature:
   - Provide information on safe crossing practices and the importance of using designated crosswalks.
   - Improve infrastructure at intersections and T-junctions to enhance pedestrian safety.
   - Install well-marked crosswalks, pedestrian signals, and signage to guide pedestrians safely across the road.
   - Consider the installation of pedestrian islands or refuge areas in the middle of wide roads to facilitate    safer crossing.
   - Conduct regular safety audits of high-risk pedestrian areas to identify and address potential hazards promptly.

### Hours_distribution:
   - Implement measures to enhance pedestrian visibility during low-light hours, especially during early morning (e.g., 5 AM - 7 AM) and evening (e.g., 6 PM - 8 PM).
   - Increase street lighting in areas with high pedestrian activity.
   - Consider installing pedestrian-activated signals or flashing beacons in areas with high pedestrian traffic.
   - Focus on pedestrian safety measures at intersections during peak accident hours.
   - Consider implementing traffic calming measures, such as raised crosswalks or refuge islands, to enhance safety
   - If accidents are prevalent during school hours, implement targeted safety measures in school zones.
   - Enhance signage and implement lower speed limits during school commuting hours.
   - Increase enforcement of speed limits during peak pedestrian accident hours to ensure drivers adhere to speed regulations.
   - Implement speed reduction measures in accident-prone areas.

### Pedestrians_age:
   - Children (0 to 16 years old):
   Implement school zone safety measures, including speed limits and enhanced crosswalks.
   Promote pedestrian safety education in schools and communities.
   Encourage parental supervision and guidance for younger pedestrians.
   - Seniors (60 years and older):
   Enhance safety features at pedestrian crossings, such as longer crossing times and audible signals.
   Collaborate with senior centers and organizations to provide pedestrian safety education tailored to older adults.
   Consider implementing traffic calming measures in areas with a significant senior population.
   - General Recommendations for All Age Groups:
   Enhance visibility for pedestrians through well-lit areas, reflective clothing, and improved street lighting.
   Promote awareness of safe pedestrian behavior, including the importance of using designated crosswalks.

# 6.Principal component analysis:
"""

from sklearn.decomposition import PCA



# Select the 3 columns to perform PCA on
cols = ['Count_Casualty_Hospitalised', 'Count_Casualty_MedicallyTreated', 'Count_Casualty_MinorInjury','Count_Casualty_Total']

# Create a PCA object
pca = PCA(n_components=4)

# Fit the PCA object to the data
pca.fit(df2[cols])

# Get the transformed data
transformed_data = pca.transform(df2[cols])

df2['Count_PCA1'] =transformed_data [:, 0]
df2['Count_PCA2'] = transformed_data[:, 1]
df2['Count_PCA3'] = transformed_data[:, 2]
df2['Count_PCA4'] = transformed_data[:, 3]
df2_PCA = df2.drop(['Count_Casualty_Hospitalised', 'Count_Casualty_MedicallyTreated', 'Count_Casualty_MinorInjury','Count_Casualty_Total'],axis=1)

print (df2_PCA.head())

df2_categorical = df2_PCA.select_dtypes(include=['object']).copy()
df2_categorical.head()

"""# 7.Build predictive models:

### 7.1.Label Encoding:
"""

# we are doing label encoding

for i in df2_categorical.columns:

    df2_PCA[i] = df2_PCA[i].astype('category').cat.codes

df2_PCA.head()

# Identify the target variable
target_variable ='Crash_Severity'

# Calculate the correlation between each variable and the target variable
correlation_matrix = df2_PCA.corr()

# Identify insignificant variables
insignificant_variables_corr = []
for column in correlation_matrix.columns:
    if abs(correlation_matrix[column][target_variable]) < 0.15:
        insignificant_variables_corr.append(column)


# Print the insignificant variables
print(insignificant_variables_corr)

from statsmodels.stats.outliers_influence import variance_inflation_factor
# Calculate the VIF values for all numerical variables
vif = pd.Series([variance_inflation_factor(df2_PCA.values, i) for i in range(df2_PCA.shape[1])], index=df2_PCA.columns)
print("hello")
vif_threshold = 5
# Identify insignificant variables
insignificant_variables = []
for column in df2_PCA.columns:
    if vif[column] > vif_threshold:
        insignificant_variables.append(column)
print(insignificant_variables)

#Removing the insignificant columns that has correlation less than 0.15 with target variable and vif value less than 5
for col in df2_PCA.columns:
    if col in insignificant_variables and col in insignificant_variables_corr:
        print(col)

# Create a list of the columns to remove
columns_to_remove = ['Crash_Type', 'Crash_Latitude','Loc_Police_Region', 'Loc_Queensland_Transport_Region', 'Loc_Main_Roads_Region', 'Crash_DCA_Code', 'Crash_DCA_Description']

# Drop the columns from df2_PCA
df2_PCA_cleaned = df2_PCA.drop(columns_to_remove,1)

print(df2_PCA_cleaned)

y=df2_PCA_cleaned['Crash_Severity']
X = df2_PCA_cleaned.drop(columns=['Crash_Severity'])

X.head()
y.head()

y.value_counts()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=0)

import sys
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split,cross_val_score,KFold
from sklearn.metrics import accuracy_score,classification_report
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression,SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier
from sklearn.svm import SVC
from scipy.stats import randint as sp_randint
from scipy.stats import uniform as sp_uniform
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate
from sklearn import metrics
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

"""# Logical Regression:"""

from sklearn.model_selection import train_test_split

# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Print the shapes of the resulting sets
print("X_train shape:", len(X_train))
print("X_test shape:", len(X_test))
print("y_train shape:", len(y_train))
print("y_test shape:", len(y_test))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize the logistic regression model
model = LogisticRegression(random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_report_str = classification_report(y_test, y_pred)

# Print the results
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", classification_report_str)

"""# KNN:"""

from sklearn.neighbors import KNeighborsClassifier
param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11]}
accuracy=[]

for i in range(1,40):
  # Initialize the KNN classifier
  knn = KNeighborsClassifier(n_neighbors=i)
  knn.fit(X_train, y_train)
  y_pred_i= knn.predict(X_test)
  accuracy.append(accuracy_score(y_test, y_pred_i))

plt.figure(figsize=(8, 6))
plt.plot(range(1,40),accuracy, color='blue', linestyle="dashed", marker="o", markerfacecolor="red", markersize=10)
plt.xlabel('K')
plt.ylabel('Accuracy')
plt.title('K vs AccuracyÂ inÂ KNN')

# Define a range of values for k (number of neighbors)
k_values = np.arange(1, 40)  # You can adjust the range as needed

# Initialize lists to store accuracy scores for training and test sets
train_scores = []
test_scores = []

# Iterate over different values of k
for k in k_values:
    # Create and train a KNN classifier with k neighbors
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    # Make predictions on the training and test sets
    y_train_pred = knn.predict(X_train)
    y_test_pred = knn.predict(X_test)

    # Calculate accuracy for the training and test sets
    train_accuracy = accuracy_score(y_train, y_train_pred)
    test_accuracy = accuracy_score(y_test, y_test_pred)

    # Append the accuracy scores to the respective lists
    train_scores.append(train_accuracy)
    test_scores.append(test_accuracy)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(k_values, train_scores, marker='o', label='Train Set')
plt.plot(k_values, test_scores, marker='o', label='Test Set')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.title('KNN Learning Curve')
plt.legend()
plt.grid(True)
plt.show()

#FINAL MODEL BUILDING

knn = KNeighborsClassifier(n_neighbors=100)


# Train the KNN classifier on the training data
knn.fit(X_train, y_train)

# Make predictions on the test data
y_pred = knn.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy {accuracy:.2f}")
knn.n_neighbors

"""AS K NEAREST NEIGHBORS IS SENSITIVE TO NOISE AND CRASH DATA HAS A NOISY ENVIRONMENT , HIGH ACCURACY CANNOT BE ACHIEVED WITH KNN"""

dt=DecisionTreeClassifier(criterion='gini', min_samples_split=300, min_samples_leaf=300)

dt.fit(X,y)
dt.feature_importances_
print(dt)
#Plot the data:
#my_colors = 'rgbkymc'  #red, green, blue, black, etc.
feature_ranks = pd.Series(dt.feature_importances_,index=X.columns)
plt.figure(figsize =(10,10))
feature_ranks.nlargest(8).sort_values(ascending=True).plot(kind='barh')

plt.show()

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
import numpy as np

# Define the model name and classifier
name = 'Decision Tree'
model = dt

# Train the model on the training data
model.fit(X_train, y_train)

# Predict on the test data
ypred = model.predict(X_test)
ypred_train = model.predict(X_train)

# Print the classification report
print(name, '\n:')
print(classification_report(y_test, ypred))
print(classification_report(ypred_train , y_train))

# Perform 5-fold cross-validation
kfold = KFold(shuffle=True, n_splits=5, random_state=0)
cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')

# Print the cross-validation results
print("%s: %f (%f)" % (name, np.mean(cv_results) * 100, np.var(cv_results, ddof=1)))

# find optimal alpha for Decision tree
path = dt.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
#Using matplotlib.pyplot to plot the effect of varying ccp_alpha on error
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1],impurities[:-1],marker='o', drawstyle="steps-post")
#blue lines drowan to highlight step changes; orange is the graph
ax.set_xlabel("Effective alpha")
ax.set_ylabel("Total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")
plt.plot(ccp_alphas,impurities)

clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker="o", drawstyle="steps-post", color = "#0097b2")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker="o", drawstyle="steps-post",  color = "#0097b2")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()
print(ccp_alphas)
print(depth)
print(ccp_alphas[9])

"""To prevent overfitting and reduce the computation time we prune the decision tree by choosing the ccp_alpha value that has lower depth and less number of nodes. To increase the accurqcy and prevent overfitting we use Kfold Cross validation method"""

train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]
print(train_scores)
print(test_scores)

fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker="o", label="train", drawstyle="steps-post", color = "red")
ax.plot(ccp_alphas, test_scores, marker="o", label="test", drawstyle="steps-post", color = "#0097b2")
ax.legend()
plt.show()

import graphviz
import sklearn.tree as tree
# DOT data
dot_data = tree.export_graphviz(dt, out_file=None,
                                feature_names=X.columns,
                                class_names='Crash_Severity',
                                filled=True)

# Draw graph
graph = graphviz.Source(dot_data, format="png")
graph

dt_final=DecisionTreeClassifier(criterion='gini', min_samples_split=300, min_samples_leaf=300,ccp_alpha=0.0026282818799708385)

# Define the model name and classifier
name = 'Decision Tree'
model = dt_final

# Train the model on the training data
model.fit(X_train, y_train)

# Predict on the test data
ypred = model.predict(X_test)
ypred_train = model.predict(X_train)

# Print the classification report
print(name, '\n:')
print(classification_report(y_test, ypred))
print(classification_report(ypred_train , y_train))

# Perform 5-fold cross-validation
kfold = KFold(shuffle=True, n_splits=5, random_state=0)
cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')

# Print the cross-validation results
print("%s: %f (%f)" % (name, np.mean(cv_results) * 100, np.var(cv_results, ddof=1)))

"""# RANDOM FOREST:"""

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
import numpy as np
rf=RandomForestClassifier(max_depth=20, min_samples_leaf=1000, min_samples_split=1000,n_estimators=50)
# Define the model name and classifier
name = 'Random forest'
model = rf

# Train the model on the training data
model.fit(X_train, y_train)

# Predict on the test data
ypred = model.predict(X_test)

# Print the classification report
print(name, '\n:')
print(classification_report(y_test, ypred))

# Perform 5-fold cross-validation
kfold = KFold(shuffle=True, n_splits=5, random_state=0)
cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')

# Print the cross-validation results
print("%s: %f (%f)" % (name, np.mean(cv_results) * 100, np.var(cv_results, ddof=1)))

from sklearn.model_selection import RandomizedSearchCV
rfrs_cv=RandomForestClassifier(max_depth=20, min_samples_leaf=1000, min_samples_split=1000,n_estimators=50)
rfrs_cv.fit(X_train,y_train)
y_train_pred=rfrs_cv.predict(X_train)
y_train_prob=rfrs_cv.predict_proba(X_train)[:,1]

y_test_pred=rfrs_cv.predict(X_test)
y_test_prob=rfrs_cv.predict_proba(X_test)[:,1]

print('Confusion Matrix-Train\n',confusion_matrix(y_train,y_train_pred))
print('Accuracy Score-Train\n',accuracy_score(y_train,y_train_pred))
print('Classification Report-Train\n',classification_report(y_train,y_train_pred))
print('\n'*2)
print('Confusion Matrix-Test\n',confusion_matrix(y_test,y_test_pred))
print('Accuracy Score-Test\n',accuracy_score(y_test,y_test_pred))
print('Classification Report-Test\n',classification_report(y_test,y_test_pred))
print('\n'*3)

# Extract one of the decision trees (e.g., the first tree)
tree_to_visualize = rf.estimators_[0]

# Export the decision tree to DOT format
dot_data1 = tree.export_graphviz(tree_to_visualize, out_file=None,
                           feature_names=X.columns,
                           class_names='Crash_Severity',
                           filled=True, rounded=True, special_characters=True)

# Create a graph from the DOT data
graph1 = graphviz.Source(dot_data1, format="png")
graph1
graph1.render("graph6",format='png', view=False)

"""Since the data has non linear patterns and relationships Random forest gives us a better accuracy of 97.6%"""